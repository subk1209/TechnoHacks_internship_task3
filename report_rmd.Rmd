---
title: "Task 3 | Employee turnover prediction"
author: "Subhajit Karmakar"
date: "2023-07-30"
output: html_document
---

<style type="text/css">
  body{
  font-size: 12pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 10, fig.height = 5)
```

## Introduction
In this data, we are provided with the information on the behaviour of the employees towards attrition. Different features are there in the data and the status of employees in terms of attrition. Here our task is to build a classification model which can predict which employees are most likely to leave the company.

**Data sourec:** <https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset>

Now we will first import the necessary libraries.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(scales)
library(grid)
library(gridExtra)
library(ROSE)
library(caTools)
library(caret)
library(rpart)
library(rpart.plot)
library(vip)
library(randomForest)


df <- read_csv("D:/Internships/Technohack/Task - 3/data.csv")
```

**Glimpse at the data**

```{r}
glimpse(df)

summary(df)
```

**Observation:** From the above summary measures, some findings are:

* The `Age` is evenly distributed.

* Some of the features have unique values only, like `EmployeeCount`, `Over18` etc.

* There are some outliers in the `MonthlyIncome`, i.e. earnings of some employees are very high.

* Also there are outliers in `TotalWorkingYears`,`YearsAtCompany`,`YearsInCurrentRole`,`YearsSinceLastPromotion`,`YearsWithCurrManager`.

**Searching for `NA` values:**

```{r}
df %>% is.na() %>% sum()
```
There is no `NA` values in the data.


#### Variables containing unique values-
From the summary tables, we can see that, some feature variables, like `EmployeeCount`, are having unique value throughout the whole data, these variables will not help anything, so we will see which variables are having this issue and then we will remove those from our data.

```{r}
# Variables with unique values:
sapply(lapply(df, unique), length) -> v

data.frame('Variables' = names(df),
           'CountOfUniques' = array(v)) %>% 
  ggplot(aes(x = Variables, y = CountOfUniques)) + 
  geom_bar(stat = 'identity', position = position_dodge(),
           width = 0.2, fill = 'blue') + theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5,
                                   hjust = 1)) +
  geom_text(aes(label = CountOfUniques), vjust = -0.2, size = 4)
```

From the above plot, we can see that, the variables `EmployeeCount`,`Over18`,`StandardHours` are having unique values, we will drop these variables.

```{r}
vars_unq <- c('EmployeeCount','Over18','StandardHours')

df <- df %>% select(!(all_of(vars_unq)))
```

## Exploratory data Analysis:
First we may be interested to see the attrition rate in the company. Also, there are categorical variables in the data, among which, some are nominal and some are of ordinal type. We might be interested to see the degree of association between those categorical variables and attrition. To judge the degree of association statistically, we will perform the $\small{\chi^2}$-test and observe the p-value.

**Findings:**
```{r}
# Attrition rate:

df %>% count(Attrition) %>% mutate('Perc' = percent(n/sum(n))) %>% 
  ggplot(aes(x = Attrition, y = n)) +
  geom_bar(stat = 'identity', position = position_dodge2(),
           width = 0.3, fill = 'yellow', colour = 'black') +
  theme_minimal() + theme(axis.title.y = element_blank()) +
  geom_text(aes(label = Perc), vjust = 2)
```

The attrition rate in the company is 16%, which is a quite high figure and also the data is highly imbalanced.


```{r}
# Categorical plots and degree of association:

plot1 <- function(var){   # function to generate the plots
  my_col <- c('orange', 'darkgreen')
  df %>% count({{var}}, Attrition) %>% 
    ggplot(aes(x = {{var}}, y = n, fill = Attrition)) +
    geom_bar(stat = 'identity', position = position_dodge2(),
             width = 0.4) + labs(y = 'Count') +
    scale_fill_manual(values = my_col) +
    theme_minimal() + scale_y_continuous(n.breaks = 7) +
    theme(legend.position = 'None', 
          axis.text.x = element_text(angle = 90, vjust = 0.5,
                                     hjust = 1)) -> p1
  
  df %>% count({{var}}, Attrition) %>% 
    ggplot(aes(x = {{var}}, y = n, fill = Attrition)) +
    geom_col(position = 'fill', width = 0.4) +
    scale_y_continuous(labels = percent, n.breaks = 7) + 
    labs(y = 'Percentage') + theme_minimal() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5,
                                     hjust = 1)) +
    scale_fill_manual(values = my_col) -> p2
  
  # Chisq-test for association
  table(var, Attrition) %>% chisq.test() -> t
  round(t$p.value, 5) -> p
  
  grid.arrange(p1,p2, ncol = 2,
    top = textGrob(paste0('p-value : ', p), 
                   gp = gpar(fontsize = 14, font = 2)))
}

attach(df)
```

```{r}
plot1(Gender)
```

**Comment:** We can see that, gender does not affect the rate of of attrition which is a quite obvious fact, and it is also evident from the p-value.

```{r}
plot1(Department)
```

**Comment:** We see that, there is a little association between department and attrition, and it is highest in case of `Sales` department. Though the association is not very much strong.

```{r}
plot1(BusinessTravel)
```

**Comment:** Attrition rate is highest in case of `Travel_Frequently`. This might be due to the fact that the employees need to travel very often which lead to the high attrition.

```{r}
plot1(JobRole)
```

**Comment:** Attrition rate is highest in case of `Sales Representative` and this rate is varing significantly accross the different job roles. It is lowest in `Research Director`. One reason can be that the sales representatives need to work hard so that the attrition rate is very high among them.

```{r}
plot1(MaritalStatus)
```

**Comment:** Attrition rate is high in case of singles. 


```{r}
plot1(OverTime)
```

**Comment:** Overtime leads to more attrition.

Now, since there are categorical variables which are ordinal, we might want to see the change in pattern of attrition rate for different levels of the variables. The observations are listed below.

```{r}
# Function to generate the graphs:

plot2 <- function(var, test){
  my_col2 <- c('green','red')
  
  df %>% count({{var}}, Attrition) %>% 
    group_by({{var}}) %>% mutate(per = percent(n/sum(n))) %>% 
    ggplot(aes(x = {{var}}, y = n, colour = Attrition)) +
    geom_line(lwd = 1) + geom_text(aes(label = per), vjust = -0.5,
                                   colour = 'blue', size = 3) +
    geom_point(colour = 'black', size = 2) +
    scale_color_manual(values = my_col2) + theme_minimal() -> a
  
  # Chisq-test for association
  if(test == 'no'){
    a + labs(y = 'Count') %>% return()
  }else if(test == 'yes'){
    table(var, Attrition) %>% chisq.test() -> t
    round(t$p.value, 5) -> p
    
    a + labs(title = paste0('p-value : ', p),
             y = 'Count') %>% return()
  }
}
```

```{r}
plot2(JobSatisfaction, test = 'yes')
```

**Comment:** As the employees are more and more satisfied with the job they have in company, the attrition rate is decreasing, which is pretty obvious thing. The attrition rate is lowest in case of the job with highest satisfaction.

```{r}
plot2(EnvironmentSatisfaction, test = 'yes')
```
**Comment:** Similar explanation is applicable in this case also. Attrition is dropping consistently with the increase in the satisfaction level in environment. Also, a lot of people are there in the company with high satisfaction level.

```{r}
plot2(Education, test = 'yes')
```
**Comment:** It seems that employees with higher education level are not more prone to leave the company, they can adapt themselves quickly to the company, so that the attrition rate is low. Also very fewer people are there in the company with highest education level. p-value indicates that education level does not affect the attrition status of the employees which is also clear from the education level 1-4. Since the frequency is very low for the highest education, that information is not helping much.

```{r}
plot2(RelationshipSatisfaction, test = 'yes')
```
**Comment:** Except `RelationshipSatisfaction = 1`, the attrition rate is constant through out the other levels, causing the attrition rate not getting affected by the relationship satisfaction.

```{r}
plot2(WorkLifeBalance, test = 'yes')
```
**Comment:** It shows that people with poor work life balance are more prone to leave the company, on the other hand, if they struggle hard to balance the work life more, the attrition rate tends to increase, so moderate work life balance seems to yield comparatively lower attrition rate.

```{r}
plot2(YearsInCurrentRole, test = 'no')
```
**Comment:** A very few people are there who stay in the company for longer time. And the attrition rate goes down as the employees stay in the company for longer period of time. No people leaves the company after 15 years of experience. 

```{r}
plot2(YearsWithCurrManager, test = 'no')
```

**Comment:** Similar explanation is applicable here like the earlier case. Some high fluctuations are there. High amount of people are leaving the company at 7 years, it might be due to the fact, after some work experience in the company, they tend to shift to a new company, so the scenario of attrition is there.



#### Continuous variables:
```{r}
plot3 <- function(var){
  my_col3 <- c('blue','yellow')
  df %>% ggplot(aes({{var}}, fill = Attrition)) +
    geom_density(alpha = 0.6, colour = NA) +
    scale_fill_manual(values = my_col3) +
    theme_minimal() -> p1
  
  df %>% ggplot(aes(x = Attrition, y = {{var}})) + 
    geom_boxplot(outlier.color = 'darkblue', outlier.size = 1,
                 outlier.shape = 4, outlier.stroke = 1.3) +
    geom_violin(alpha = 0.4, color = NA, fill = 'green') +
    coord_flip() + theme_minimal() + labs(x = '')  -> p2
  
  grid.arrange(p1,p2, ncol = 2)
}
```


```{r}
plot3(MonthlyIncome)
```
**Comment:** Among those who left the company, a large number of people had very low income. Very few outliers are there in case of those who left. Both the distributions are positively skewed which implies that very few people were given high paid jobs.


```{r}
plot3(Age)
```
**Comment:** The young people are more prone to leave the company, one reason might be that they try to explore the other job options.


In the following plot, we will see the distribution of monthly income with respect to two important categorical variables. 
```{r}
# Overtime ~ Salary
df %>% group_by(OverTime, Attrition) %>% 
  summarise(MonthlyIncome = round(mean(MonthlyIncome))) -> avg_df

df %>% ggplot(aes(x = OverTime, y = MonthlyIncome, 
                  fill = Attrition)) + 
  geom_boxplot(outlier.colour = 'darkblue', outlier.shape = 4,
               outlier.stroke = 1.3, outlier.alpha = 0.5) +
   geom_text(data = avg_df, aes(label = MonthlyIncome),
             position = position_dodge(width = 0.8)) +
  theme_minimal() + scale_y_continuous(labels = 
            label_number(suffix = "K", scale = 1e-3),
            n.breaks = 10)
```
The average salary is very low for those who did overtime and left the company, compared to the other categories, this can be a big reason for them to left the company. This plot is very informative in the sense that it gives us an clear idea about the attrition status of the employees who did overtime.


## Model building:
Now, the task is to fit a model which can predict which employees are most likely to leave the company. Now the fact is, since the data is imbalanced, the model to be fitted might be biased one, to ignore this issue, we will balance the data later.

Here the model we are interested to fit is: **Decision tree**

Also, to ignore the problem of over-fitting, we will use K-fold cross validation. The whole procedure is given below.

**Fitting the model:**
```{r}
# Splitting the data:
set.seed(42)
s <- sample.split(df$Attrition, SplitRatio = 3/4)
train_data <- df[s,]
test_data <- df[!s,]

# K-fold cross validation:
folds <- createMultiFolds(train_data$Attrition, k = 5, times = 5)
control <- trainControl(method = "repeatedcv", index = folds)
classifier_cv <- train(Attrition ~ ., 
                       data = train_data, method = "rpart", 
                       trControl = control)

# final model
classifier_cv$finalModel -> dtree_f 
```
**Visualizing the tree**
```{r}
par(mfrow = c(1,2))
rpart.plot(dtree_f, extra = 4)
prp(dtree_f)
```


Now we will see the performance of the model.
```{r}
# Function for performance metrics
stats2 <- function(C){
  t <- C$table
  
  acc <- C$overall[[1]]
  pre <- t[2,2]/(t[2,2]+t[2,1])
  rec <- t[2,2]/(t[2,2]+t[1,2])
  f1 <- 2*(rec*pre)/(rec+pre)
  
  matrix(c(acc,pre,rec,f1), byrow = T,
         dimnames = list(c('Accuracy','Precision',
                           'Recall','F1-Score'))) -> M
  return(list('Confusion Matrix' = t,
              'Metrics' = M))
}

  # For training data:
attr_pred_train <- predict(classifier_cv, newdata = train_data)
confusionMatrix(as.factor(train_data$Attrition), 
                attr_pred_train) -> C1
stats2(C1)


  # For testing data:
attr_pred_test <- predict(classifier_cv, newdata = test_data)
confusionMatrix(as.factor(test_data$Attrition), 
                attr_pred_test) -> C2
stats2(C2)
```

**Comment:** We see that, though the accuracy is very high in both the cases (training and testing set), the F1-score is very low, also this fact is clear from the contingency table. The false positive rate is very high, i.e. the model is predicting `Yes` most of the time, which indicates that it is a biased model. So we will now balance the data and fit the model again to resolve this issue. 

**Balancing the data**
```{r}
df_o <- ovun.sample(Attrition~., data = df,
                    p = 0.5, method = 'over', seed = 1)$data %>% 
  as_tibble()


  # dimension of new data
f <- function(x)(paste(x[1],'X',x[2]))
glue::glue("Dimension of new data: {d}",
           d = dim(df_o) %>% f)
```

**Attrition rate in new data:**
```{r}
df_o %>% count(Attrition) %>% 
  mutate(Percentage = percent(n/sum(n)))
```

#### Re-fitting the model:
```{r}
# Splitting the data:
set.seed(42)
s <- sample.split(df_o$Attrition, SplitRatio = 3/4)
train_data2 <- df_o[s,]
test_data2 <- df_o[!s,]

# K-fold cross validation:
folds <- createMultiFolds(train_data2$Attrition, k = 5, times = 5)
control <- trainControl(method = "repeatedcv", index = folds)
classifier_cv <- train(Attrition ~ ., 
                       data = train_data2, method = "rpart", 
                       trControl = control)

# final model
classifier_cv$finalModel -> dtree_f2 
```
**Visualizing the tree**
```{r}
par(mfrow = c(1,2))
rpart.plot(dtree_f2, extra = 4)
prp(dtree_f2)
```

**Performance of the new model**
```{r}
  # For training data:
attr_pred_train2 <- predict(classifier_cv, newdata = train_data2)
confusionMatrix(as.factor(train_data2$Attrition), 
                attr_pred_train2) -> C1
stats2(C1)


  # For testing data:
attr_pred_test2 <- predict(classifier_cv, newdata = test_data2)
confusionMatrix(as.factor(test_data2$Attrition), 
                attr_pred_test2) -> C2
stats2(C2)
```

**Comment:** The accuracy has decreased but the classification is quite good than the earlier case. The F1-score is more than 0.5 which indicates that the model is good. 


# Thanks for your patience!
